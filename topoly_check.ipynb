{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topoly_check.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlGkbnmrMfUfGwZ4hYsm0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dav3794/CeNT_internship_Dawid_Uchal/blob/master/topoly_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "8IIYW6Ki9iV2",
        "outputId": "c3aece37-a1d7-4c73-fba9-4bf1938c4a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting topoly\n",
            "  Downloading topoly-0.9.25-cp37-cp37m-manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from topoly) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from topoly) (3.2.2)\n",
            "Collecting argparse>=1.4\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting biopython>1.60\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from topoly) (1.21.6)\n",
            "Requirement already satisfied: Pillow<=8.3.2 in /usr/local/lib/python3.7/dist-packages (from topoly) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->topoly) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->topoly) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->topoly) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->topoly) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->topoly) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->topoly) (1.15.0)\n",
            "Installing collected packages: biopython, argparse, topoly\n",
            "Successfully installed argparse-1.4.0 biopython-1.79 topoly-0.9.25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=86840d8f6d4fc275565c10afcfec9222ed6141a8f0382a13df496ffd74c4d46e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install topoly\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fasta_to_json(file, save_json=0):\n",
        "    header = ['id', 'sequence']\n",
        "    data = {}\n",
        "    seq_temp = \"\"\n",
        "\n",
        "    with open(f\"{file}_full_length_sequences.fasta\", \"r\") as input_file:\n",
        "        for line in input_file.readlines():\n",
        "            if line[0] == \">\":\n",
        "                if len(seq_temp) > 0:\n",
        "                    data.update({entry: seq_temp})\n",
        "                seq_temp = \"\"\n",
        "                pattern = '>.*? '\n",
        "                match_results = re.search(pattern, line, re.IGNORECASE)\n",
        "                entry = match_results.group()\n",
        "                entry = re.sub(\">\", \"\", entry)\n",
        "                entry = entry.strip()\n",
        "            else:\n",
        "                seq_temp += line.strip()\n",
        "        else:\n",
        "            data.update({entry: seq_temp})\n",
        "            #print(\"Done\")\n",
        "\n",
        "    if save_json:\n",
        "        json_dict = json.dumps(data)\n",
        "        with open(f\"{file}_sequences.json\", \"w\") as f:\n",
        "            f.write(json_dict)\n",
        "            print(f\"Saved to {file}_sequences.json\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def atomize_clusters(file, save=True):\n",
        "    cwd = os.getcwd()\n",
        "    dir = os.path.join(cwd, f\"{file}_clusters\")\n",
        "    if not os.path.exists(dir):\n",
        "        os.mkdir(dir)\n",
        "\n",
        "    seq_dict = fasta_to_json(f\"PF{file}\")\n",
        "    out = []\n",
        "    i = 0\n",
        "    with open(f\"{file}.clstr\", \"r\") as f:\n",
        "        for line in tqdm(f.readlines()):\n",
        "            if line[0] == \">\":\n",
        "                if i > 0:\n",
        "                    if save:\n",
        "                        pd.DataFrame(clstr).drop_duplicates().to_csv(f\"{dir}/Cluster {i-1}.txt\", sep=\",\", header=[\"id\", \"sequence\"], index=False)\n",
        "                    else:\n",
        "                        out.append(clstr)\n",
        "                i += 1\n",
        "                clstr = []\n",
        "            else:\n",
        "                items = line.split()\n",
        "                entry = items[2][1:-3]\n",
        "                row = [[entry, seq_dict[entry]]]\n",
        "                if items[-1] == \"*\":\n",
        "                    clstr = row + clstr\n",
        "                else: clstr += row\n",
        "        else:\n",
        "            if save:\n",
        "                pd.DataFrame(clstr).drop_duplicates().to_csv(f\"{dir}/Cluster {i-1}.txt\", sep=\",\", header=[\"id\", \"sequence\"], index=False)\n",
        "            else:\n",
        "                out.append(clstr)\n",
        "                return out\n"
      ],
      "metadata": {
        "id": "hAX-V1UZRCQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import multiprocessing.pool\n",
        "\n",
        "class NoDaemonProcess(multiprocessing.Process):\n",
        "    # make 'daemon' attribute always return False\n",
        "    def _get_daemon(self):\n",
        "        return False\n",
        "    def _set_daemon(self, value):\n",
        "        pass\n",
        "    daemon = property(_get_daemon, _set_daemon)\n",
        "\n",
        "class MyPool(multiprocessing.pool.Pool):\n",
        "    Process = NoDaemonProcess"
      ],
      "metadata": {
        "id": "l4tbfOK1a2m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import shutil\n",
        "from topoly import alexander, homfly\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "def get_pLDDT(path):\n",
        "    with open(path, \"r\") as f:\n",
        "              cif = f.read()\n",
        "              pattern = 'global\\.metric_value .*?\\n'\n",
        "              match_results = re.search(pattern, cif, re.IGNORECASE)\n",
        "              entry = match_results.group()\n",
        "              return float(entry.split()[1])\n",
        "\n",
        "\n",
        "def calculate_topology(file, print_knot = True, from_saved = True, use_matrix=False):\n",
        "    if from_saved:\n",
        "        df = pd.read_csv(file, sep=\",\", header=0)\n",
        "        df = df.to_numpy()\n",
        "    else:\n",
        "        df = file\n",
        "\n",
        "    if not os.path.exists(\"./data/\"):\n",
        "      os.mkdir(\"./data/\")\n",
        "\n",
        "    for id, sequence in df:\n",
        "        try:\n",
        "          id_AF = re.sub(\"_.*?$\", \"\", id)\n",
        "          path = f\"./data/{id_AF}.cif\"\n",
        "          url = f\"https://alphafold.ebi.ac.uk/files/AF-{id_AF}-F1-model_v3.cif\"\n",
        "          if not os.path.exists(path):\n",
        "            wget.download(url, out=path)\n",
        "\n",
        "          if get_pLDDT(path) > 0.5:\n",
        "              if use_matrix:\n",
        "                  knot = alexander(path, tries=50, matrix=True, density=3)\n",
        "              else:\n",
        "                  knot = homfly(path, max_cross=50, translate=True)\n",
        "\n",
        "              knot_type = \"01\"\n",
        "              if \"0_1\" in knot:\n",
        "                  if knot[\"0_1\"] > 0.5:\n",
        "                      if print_knot:\n",
        "                          print(id, \"unknot\")\n",
        "                  else:\n",
        "                      tops = list(knot.keys())\n",
        "                      tops.sort(key=lambda i: knot[i], reverse=True)\n",
        "                      knot_type = re.sub('[^A-Za-z0-9]+', '', tops[0])\n",
        "                      if print_knot:\n",
        "                          print(id, \"knot: \" + knot_type)\n",
        "                  return [id, sequence, knot_type]\n",
        "        except: \n",
        "            if print_knot:\n",
        "                print(\"Error occured\", id)\n",
        "            else: pass\n",
        "\n",
        "\n",
        "def get_topology(family_id, multiprocess = 1, seed = 0, use_matrix=False):\n",
        "    if multiprocess:\n",
        "        print(\"Atomizing clusters to files:\")\n",
        "        atomize_clusters(family_id)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        files = os.path.join(f\"./{family_id}_clusters/\", \"Cluster *.txt\")\n",
        "        files = glob(files)\n",
        "\n",
        "        if seed:  files = files[:seed]\n",
        "\n",
        "        print(\"Calculating topology for protein:\")\n",
        "        with MyPool(10) as p:\n",
        "            data = p.map(calculate_topology, files)\n",
        "\n",
        "        data = list(filter(None, data))\n",
        "        \n",
        "        print(\"Deleting redundant data.\")\n",
        "        dir_path = f'./{family_id}_clusters/'\n",
        "        try:\n",
        "            shutil.rmtree(dir_path)\n",
        "            #shutil.rmtree('./data/')\n",
        "            print(\"Data removed.\")\n",
        "        except OSError as e:\n",
        "            print(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
        "\n",
        "    else:\n",
        "        print(\"Atomizing clusters.\")\n",
        "        out = atomize_clusters(family_id, 0, 0)\n",
        "        if seed:   out = out[:seed]\n",
        "        data = []\n",
        "\n",
        "        print(\"Calculating topology for proteins:\")\n",
        "        for file in tqdm(out):\n",
        "            res_tmp = calculate_topology(file, 0, use_matrix = use_matrix)\n",
        "            if res_tmp: data.append(res_tmp)\n",
        "\n",
        "    print(\"Saving calculated topology.\")\n",
        "    pd.DataFrame(data).to_csv(f\"{family_id}_topology.csv\", sep=\",\", header=[\"id\", \"seq\", \"type_of_knot\"], index=False)\n",
        "    print(f\"Topology saved as {family_id}_topology.csv\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "BZLBDy6-EOsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do prawidłowego uruchomienia należy w miejscu wykonywania programu umieścić plik \"PF`{family_id}`_full_length_sequences.fasta\" oraz plik wyjściowy .clstr z CD-HIT nazwany \"`{family_id}`.clstr\"."
      ],
      "metadata": {
        "id": "YCrm3Ark-Bt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    data = get_topology(\"04013\")"
      ],
      "metadata": {
        "id": "IFCoK-Topt3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r dane.zip ./data"
      ],
      "metadata": {
        "id": "Wt2EvG2fXZSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DYDsG-nmM6Gz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}